---
layout: page
title: Hemlata
subtitle: Senior Research Scientist- Audio Deepfake Detection
use-site-title: true
---

<br> Hi! I'm Hemlata Tak, a Senior Research Scientist at Pindrop, USA, with a Ph.D. in Computer Science from Sorbonne University, France, where I worked under the guidance of Prof. Nicholas Evans. My expertise lies in the voice authentication, deepfake detection, AI singing detection, and model attribution.<br><br>

<h3>Professional Impact</h3>
In my role at Pindrop, I apply my expertise in machine learning and speech foundational models to build impectful deepfake detection solutions that improve detection accuracy in real world conditions. Key contribution include:<br><br>

<ul>
	<li><i> <b>Pindrop Pulse for Meetings</b>: One of the TIME's Best Inventions of 2025 <a href="https://time.com/collections/best-inventions-2025/7318241/pindrop-pulse-for-meetings/">award </a>. I operationalized a deep learning based deepfake detection system with enhanced detection performance across call centre, multimedia and real-time meeting scenarios</a>.
	</li><br>
	<li><i> Contributed in ACM Multimedia 2025 1M-Deepfake Detection Challenge and <a href="https://deepfakes1m.github.io/2025/evaluation"> won the challenge in the Deepfake Classification and Localization tasks </a>.
	</li><br>
</ul>
This experience reflects my commitment to leveraging advanced  AI and machine learning to solve complex, large-scale challenges and to drive continuous improvement in deepfake detection performace.<br><br>

<h3>Updates:</h3>

<ul>
	<li><i> October 2025: We're proud to announce that our product <b>Pindrop Pulse for Meetings</b> has been awared one of <b>TIME's Best Inventions of 2025</b> <a href="https://time.com/collections/best-inventions-2025/7318241/pindrop-pulse-for-meetings/">award </a>. I'm proud to be part of the excellent team to build such a transformative solution to detect fraud. Check out <a href="https://www.pindrop.com/product/pindrop-pulse/meetings">here</a>.
	</li><br>
	<li><i>August 2025: One paper accepted in ACM multimedia 2025 conference on Audio-visual deepfake detection and localization</a>.
	</li><br>
	<li><i>July 2025: We’re excited to announce that we secured 1st place in Task 2 deepfake localization, and 2nd place in Task 1 deepfake detection <a href="https://deepfakes1m.github.io/2025/evaluation"> of the ACM Multimedia 2025 1M-Deepfake Detection Challenge on TestA and most challenging hidden TestB set</a>.
	</li><br>
	<li><i>June 2025: We’re excited to announce two special sessions at IEEE ASRU 2025 workshop: <b>Responsible Speech and Audio Generative AI</b> and <b>Frontiers in Deepfake Voice Detection and Beyond</b>. These sessions will explore cutting-edge research in Trustworthy AI, including themes of fairness, interpretability, and robust deepfake detection. We look forward to insightful discussions and meaningful collaborations at ASRU 2025 conference! Learn more <a href="https://2025.ieeeasru.org/program/special-sessions">here</a>.
	</li><br>
	<li><i>May 2025: Im thrilled to announce that we are organizing a special session on <b>Source Tracing for Audio Deepfake Detection</b> at Interspeech 2025 conference - marking the first time this important and emerging topic will be featured at the interspeech conference. This session has received 18 papers and 11 high quality papers were accepted. These contribution explore cutting-edge techniques and novel approaches to identifying the origin of deepfake audio and strengthening defenses against deepfake attacks. Join us as we take a significant step forward in advancing the field of trustworthy AI <a href="https://www.interspeech2025.org/special-sessions">. Find details here</a>.
	</li><br>
	<li><i>May 2025: one paper accepted in Interspeech 2025 </a>.
	</li><br>
	<li><i>Jan 2025: Honored to receive the <b>Rookie of the Year 2024</b> award at Pindrop </a>.
	</li><br>
	<li><i>Jan 2025: The ASVspoof5 audio deepfake dataset and ground truths are now all freely available to the community. Check it out <a href="https://zenodo.org/records/14498691">Dataset</a> and <a href="https://arxiv.org/html/2502.08857v2">paper</a>.
	</li><br>
	<li><i>Jan 2025: Organising a special session on <b> Source tracing: The origins of synthetic or manipulated speech </b> at Interspeech 2025 <a href="https://www.interspeech2025.org/special-sessions">. Papers on relevant topics are welcome in a special session </a>.
	</li><br>
	<li><i>Jan 2025: Paper accepted at ICASSP 2025.
	</li><br>
	<li><i>July 2024: Organising ASVspoof 2024 workshop as Interspeech 2024 satellite event <a href="https://www.asvspoof.org/workshop2024">. Registrations are open</a>.
	</li><br> 	
	<li><i>June 2024: Three papers accepted at <a href="https://interspeech2024.org/">InterSpeech 2024</a>.
	</li><br> 
	<li><i>May 2024:</i> ASVspoof5 detection challenge (phase2) is now started. Check it out <a href="https://www.asvspoof.org/">here</a>.
	</li><br>	
	<li><i>January 2024:</i> I joined Pindrop as a Research Scientist! I will be working
		on Audio deepfake detection.
	</li><br>
	
	<li><i>Nov 2023:</i> I will be delivering a talk with Prof. Massimiliano Todisco (EURECOM) on "From Artefacts to Insights: A Topical Analysis of Voice Biometric Security" at the Joint Workshop of VoicePersonae and ASVspoof 2023, Tokyo, Japan. For more information check it out <a href="https://nii-yamagishilab.github.io/workshops/voicepersonae/">here</a>.
	</li><br> 
	<li><i>August 2023:</i> Our paper entitled "t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators", accepted in IEEE transactions on pattern analysis and machine intelligence (TPAMI) 2023.
	</li><br>
	<li><i>August 2023:</i> I will be offering a tutorial on "Advances in audio anti-spoofing and deepfake detection using graph neural networks and self-supervised learning" at INTERSPEECH 2023 Conference, Dublin, Ireland.
	</li><br>
        <li><i>July 2023:</i> ASVspoof5 is now calling for spoofed speech data contributors. Check it out <a href="https://www.asvspoof.org/">here</a>.
	</li><br>
	<li><i>May 2023: Two papers accepted at <a href="https://www.interspeech2023.org/">InterSpeech 2023</a>.
	</li><br> 
